{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ef6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec610f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613349df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3523dd",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## We'll start with building **bigram LM**\n",
    "    - Simple & weak LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd643af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting individual character bigrams\n",
    "\n",
    "for w in words[:3]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "#     for ch1, ch2 in zip(w, w[1:]):\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ac911",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab693e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "['<S>'] + list(w) + ['<E>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30331d4b",
   "metadata": {},
   "source": [
    "- In order to learn the statistics of which characters are likely to follow other characters:\n",
    "    - Simplest way is to do by counting (in **bigram LM**)\n",
    "        - How often any of these combination occurs in the dataset?\n",
    "        \n",
    "- We can think of a dictionary, which can maintain counts of bigrams.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # .get returns 0 if the key does not exist in dict\n",
    "        \n",
    "#         print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d0eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.items()\n",
    "# return tuples of (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54be6c",
   "metadata": {},
   "source": [
    "- Now we want to sort the above tuples of (key, value)\n",
    "- But by default ```sorted``` sorts on 1st item of tuple.\n",
    "    - Whereas, we want to sort them by values, which are 2nd item of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936274f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key = lambda kv: -kv[1])\n",
    "# this lambda takes (key, value) tuple and returns the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5b0db",
   "metadata": {},
   "source": [
    "- Now it would be better to keep this information in a 2-d array instead of python dict.\n",
    "    - **rows** would be the 1st character\n",
    "    - **columns** would be the 2nd character, following 1st.\n",
    "    - the **entries** will give us the count of occurence of 1st character followed by 2nd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555331b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a tensor with int32 type, for 28x28 size\n",
    "# 26 alphabets + 2 special characters\n",
    "\n",
    "N = torch.zeros((28,28), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709d907",
   "metadata": {},
   "source": [
    "- We have characters that are strings,\n",
    "- But to build the 2-d array we might need to convert them to numbers\n",
    "- To do so, we would require a lookup table - **from characters to integers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edcd65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd3a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(''.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i for i,s in enumerate(chars)} # mapping\n",
    "stoi['<S>'] = 26\n",
    "stoi['<E>'] = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377de30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56718c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3280281",
   "metadata": {},
   "outputs": [],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94207b5",
   "metadata": {},
   "source": [
    "### Visualizing the array 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe953f1",
   "metadata": {},
   "source": [
    "#### Even better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40852d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        chstr = itos[i] + itos[j] # character string - bigrams in character representation\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray') # plot the bigram text\n",
    "        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray') # plot the no. of times a bigram occurs\n",
    "\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488c1ce",
   "metadata": {},
   "source": [
    "#### In the above viusal we see - char in row followed by char in column with their counts\n",
    "    - Some of them occur often, and some do not\n",
    "    - Consider the last row - end of word **<E>** followed by a character does not make sense\n",
    "        - Also shown by 0s in the entire row\n",
    "    - Consider the 2nd last column - start of a word **<S>** following a character does not make sense\n",
    "        - Also shown by all 0s in the entire column\n",
    "    - Another unintuitive thing - looking at last 2x2 matrix, you see <S> & <E> occuring together in any order does not make sense.\n",
    "    \n",
    "    - Using <S> and <E> makes the visual crowded.\n",
    "    \n",
    "**Therefore we make some changes considering the above points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06691f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27), dtype=torch.int32) # We're going to have only 1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52222d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # mapping\n",
    "stoi['.'] = 0 # The only special character, but we move it to 0, just to make it pleasing; And offset remaining char\n",
    "# stoi['<E>'] = 27\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc032d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7284e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j] # character string - bigrams in character representation\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray') # plot the bigram text\n",
    "        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray') # plot the no. of times a bigram occurs\n",
    "\n",
    "plt.axis('off');\n",
    "\n",
    "# Counts array of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at how often a character is starting the name word\n",
    "N[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p/p.sum() #Normalized and converted to probabilities\n",
    "p #Probability of any character to be the first character in a word/name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8df454",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f523463",
   "metadata": {},
   "source": [
    "- Now we sample from above distribution\n",
    "    - using pytorch **multinomial**; you give me prob - I'll give you integers (which are sampled according to the distribution)\n",
    "    - pytorch **generator**; to make everything deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2228f",
   "metadata": {},
   "source": [
    "############################--------------------------################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "p = torch.rand(3, generator=g)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f438c",
   "metadata": {},
   "source": [
    "- Now we use ```torch.multinomial``` to draw sample from above distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(p, num_samples=20, replacement=True, generator=g)\n",
    "\n",
    "# What this will do is give us numbers 0,1,2 in ratio that is seen in \"p\" distribution\n",
    "# Around 60% of num_samples will be 0, 30% will be 1, and only 10% are 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea5a1e",
   "metadata": {},
   "source": [
    "############################--------------------------################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get back to our original p distribution\n",
    "p = N[0].float()\n",
    "p = p/p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "# We've sampled the 1st character of our word\n",
    "ix, itos[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0ef6c",
   "metadata": {},
   "source": [
    "#### As you can infer from above visual, 'm' starts 2538 words out of total 32000 words (approx 10% of the total)\n",
    "\n",
    "- **m** is already sampled, we now proceed with further characters\n",
    "- To draw next, we jump to the row of **m**, and sample from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297728c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we write a loop to sample word\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        p = N[ix].float()\n",
    "        p = p/p.sum()\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cde7f5",
   "metadata": {},
   "source": [
    "#### Obviously, the names above hardly sound name-like\n",
    "- Probably because the **bi-gram** model is very weak\n",
    "    - Also, it only knows what is 1 step ahead of it while generating, doesn't matter it it's the first word or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc4c2a",
   "metadata": {},
   "source": [
    "#### To justify if it's at least doing something sensible generation:\n",
    "    - We try to generate with equaly likely distribution (uniform)\n",
    "    - This does even worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we write a loop to sample word\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "#         p = N[ix].float()\n",
    "#         p = p/p.sum()\n",
    "        p = torch.ones(27) / 27.0\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270e596",
   "metadata": {},
   "source": [
    "## Removing in-efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877902ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "N[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "# P:                           27, 27\n",
    "# P.sum(axis=1, keepdim=True): 27,  1\n",
    "\n",
    "# eligible for broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(axis=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to convert entire \"N\" tensor into float; and divide every row elements by its sum (sum of ROW)\n",
    "P /= P.sum(axis=1, keepdim=True) #Denominator gets stretched from 1 to 27 columns; with BROADCASTING; and then do elementwise division\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba4f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we write a loop to sample word\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        \n",
    "        # These two lines of code below are very inefficient, doing same operation everytime\n",
    "#         p = N[ix].float()\n",
    "#         p = p/p.sum()\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e49614",
   "metadata": {},
   "source": [
    "## Something SCARY about broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n",
    "P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c954576",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92141762",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b48e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting\n",
    "# P:                           27, 27\n",
    "# P.sum(axis=1):                   27\n",
    "\n",
    "# Broadcast will create an axis in 1 position, and will even do elementwise division,\n",
    "# But this is a BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26726561",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P / P.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0].sum() # This is wrong, it went on to normalize the columns instead of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[:, 0].sum() # Wrongly, columns got normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ae078",
   "metadata": {},
   "source": [
    "#### Scary part ends ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68968c5c",
   "metadata": {},
   "source": [
    "## Evaluating the bi-gram model quality - loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15345b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we write a loop to sample word\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximize likelihood of the data w.r.t. model parameters (statistical modeling)\n",
    "# equivalent to maximizing the log likelihood (because log is monotonic)\n",
    "# equivalent to minimizing the negative log likelihood\n",
    "# equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e60a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "# In case of all equally likely probabilities, a probability of ~4% (1/27) would be called untrained.\n",
    "# But as you see in prob printed here, some are as high as 40%,\n",
    "# we can say our bi-gram model has certainly learned something.\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a92cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not how do we use above shown probabilities, to judge quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a149b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the average nll for entire dataset\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "#         print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "# In case of all equally likely probabilities, a probability of ~4% (1/27) would be called untrained.\n",
    "# But as you see in prob printed here, some are as high as 40%,\n",
    "# we can say our bi-gram model has certainly learned something.\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding nll for a specific word - \"mrigank\"\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in ['mrigank']:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "# In case of all equally likely probabilities, a probability of ~4% (1/27) would be called untrained.\n",
    "# But as you see in prob printed here, some are as high as 40%,\n",
    "# we can say our bi-gram model has certainly learned something.\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')\n",
    "\n",
    "# Average nll for \"mrigank\" = 2.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what if we add add a 'g' in the end\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in ['mrigankg']:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "# In case of all equally likely probabilities, a probability of ~4% (1/27) would be called untrained.\n",
    "# But as you see in prob printed here, some are as high as 40%,\n",
    "# we can say our bi-gram model has certainly learned something.\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')\n",
    "\n",
    "# Average nll for \"mrigank\" = inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1158ce",
   "metadata": {},
   "source": [
    "### The average nll becomes - inf\n",
    "    - Reason: count of the bigram ```kg``` is 0\n",
    "    - This can be verified from the last visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb0ba4",
   "metadata": {},
   "source": [
    "## To handle above problem - we do Model Smoothing\n",
    "    - 1 is added to all numbers in matrix N\n",
    "    - This is called - adding some fake counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in ['mrigankg']:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # Map both ch1 & ch2 to their integers\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "\n",
    "# In case of all equally likely probabilities, a probability of ~4% (1/27) would be called untrained.\n",
    "# But as you see in prob printed here, some are as high as 40%,\n",
    "# we can say our bi-gram model has certainly learned something.\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')\n",
    "\n",
    "# Its not a zero any longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N+1, cmap=\"Blues\")\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j] # character string - bigrams in character representation\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray') # plot the bigram text\n",
    "        plt.text(j, i, (N+1)[i, j].item(), ha='center', va='top', color='gray') # plot the no. of times a bigram occurs\n",
    "\n",
    "plt.axis('off');\n",
    "\n",
    "# Counts array of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eed07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One last time, sampling characters for word\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        p = P[ix] #P is smoothened here\n",
    "        \n",
    "        # These two lines of code below are very inefficient, doing same operation everytime\n",
    "#         p = N[ix].float()\n",
    "#         p = p/p.sum()\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8499a0",
   "metadata": {},
   "source": [
    "####################################---------------------------------------######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb55cfe",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## We build neural network\n",
    "    - Given the 1st character of bigram, we predict next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d31db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set of bigrams (x, y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # torch.Tensor would give float values\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278ce37",
   "metadata": {},
   "source": [
    "#### We should not directly feed in these integers to the network\n",
    "    - Simplest way of encoding them is One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c11a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69105b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d68d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc @ W      # matrix multiplication operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9149c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W)[3, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1bf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e52a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ada7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc[3] * W[:, 13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "\n",
    "# These 2 steps combine to make \"SoftMax\"\n",
    "counts = logits.exp() # equivalent to the N matrix\n",
    "probs = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a685e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c1bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs[0]\n",
    "\n",
    "# The 1st character (\".\") of 1st bigram (\".e\"), is converted to \"int\" and encoded with one-hot\n",
    "# then fed into neural network\n",
    "# Out came a distribution of 27 (# of neurons) probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72227f3",
   "metadata": {},
   "source": [
    "### Now we have to optimize \"W\", such that probabilities coming out are pretty good\n",
    "    - To do so, we use \"loss function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0973812",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram\n",
    "    x = xs[i].item() # input character index\n",
    "    y = ys[i].item() # label character index\n",
    "    \n",
    "    print('----------------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} indexes ({x},{y})')\n",
    "    print('input to neural nets:', x)\n",
    "    print('output probabilities form neural nets:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    \n",
    "    p = probs[i,y]\n",
    "    print('probability assigned by the net to the correct character:', p.item())\n",
    "    \n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood: ', logp)\n",
    "    \n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll)\n",
    "    nlls[i] = nll\n",
    "    \n",
    "print('\\n================\\n')\n",
    "print('Average nll, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa67f6a",
   "metadata": {},
   "source": [
    "### Now, we have to minimize this loss by tuning W with backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d689886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- OPTIMIZATION !!  GRADIENT DESCENT     ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization - random\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W # log-counts\n",
    "\n",
    "# These 2 steps combine to make \"SoftMax\"\n",
    "counts = logits.exp() # equivalent to the N matrix\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "\n",
    "# loss calculation\n",
    "\n",
    "# We would want to extract probabilities of correct index from \"probs\"; but below is inefficient way\n",
    "# probs[0,5], probs[1,13], probs[2,13], probs[3,1], probs[4,0]\n",
    "\n",
    "# efficient way\n",
    "# probs[torch.arange(5), ys]\n",
    "\n",
    "# Average - negative - log - likelihood\n",
    "loss = -probs[torch.arange(5), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "W.grad = None # set the grads to 0\n",
    "# similar to doing W.grad = 0\n",
    "\n",
    "loss.backward()\n",
    "# this fills in the gradients of all intermediate steps in the computational graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W.grad\n",
    "\n",
    "# every element here tells us:\n",
    "# influence of that weight on loss\n",
    "# If +ve: means it will nudge the loss positive direction if increased.\n",
    "# If -ve: nudge the loss lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad.shape, W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- EVERYTHING TOGETHER -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7df350",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs) # torch.Tensor would give float values\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of examples: \", num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4d51c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = logits.exp() # equivalent to the N matrix\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(\"loss: \", loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set the grads to 0\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c618c22",
   "metadata": {},
   "source": [
    "## Few notes: on smoothing\n",
    "\n",
    "1. In bi-gram approach, smoothing was done by adding a number (1) to the counts in N matrix.\n",
    "    - Instead if we would have added a very large number (say 1,000,000), all counts would have become relatively equal.\n",
    "    - And every bigram would have become equally likely.\n",
    "    - Uniform distribution\n",
    "\n",
    "2. Gradient based framework - has an equivalent to smoothing\n",
    "    - Suppose, if **W** was initialized as all 0.\n",
    "        - All **logits** would have become 0.\n",
    "            - All **counts** would have become 1.\n",
    "                - **probs** would have turned out to be **uniform**\n",
    "\n",
    "    - Trying to incentivizing **W** near 0;\n",
    "        - Is equivalent to label smoothing\n",
    "            - The more you incentivize that, more **smooth** distribution you'll achieve\n",
    "\n",
    "## Few notes: on Regularization\n",
    "\n",
    "1. We can augment the loss function\n",
    "    - To have a small component called **regularization loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67099d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "(W ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c1ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gradient descent\n",
    "\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W # log-counts\n",
    "    counts = logits.exp() # equivalent to the N matrix\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    \n",
    "    # Adding this regularization component - pushes optimization towards zeroing of the W\n",
    "    # Adding larger number to bi-gram approach - relates to increasing 0.001 term below\n",
    "    # The more you increase 0.001, the regualrization term dominates the 1st part\n",
    "    # ==> The more W weights will be unable to grow, so everything will be kind of uniform prediction\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + (0.001 * (W ** 2).mean())\n",
    "    print(\"loss: \", loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set the grads to 0\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c2610",
   "metadata": {},
   "source": [
    "### Sampling form Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7066fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "        # Before\n",
    "        p = P[ix] #P is smoothened here\n",
    "        \n",
    "        # Now\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # log-counts\n",
    "        counts = logits.exp() # equivalent to the N matrix\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These names generations are exactly same as one from bi-gram model\n",
    "# same loss\n",
    "# W is equivalent to log-counts; analogous to the N matrix\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e7a173ccb88ceeb696d2dc9aac178766b899d652568ecc857451e26796a558b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
