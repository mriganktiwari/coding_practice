{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757dc797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad18613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    \n",
    "    print(w)\n",
    "    context = [0] * block_size # padding with .'s of block_size count\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '----->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop & append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d348d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fe3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0072c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9214d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to map 27 characters into a 2 dimensional space\n",
    "\n",
    "C = torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac173c",
   "metadata": {},
   "source": [
    "#### Before trying to embed all integers in \"X\", let's try embedding a single integer, say - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way could be just index into 5th row of C\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other way could be doing \n",
    "# \"dot\" multiplication of One-Hot vector of 5 (made with 27 classes)\n",
    "# with C\n",
    "\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C\n",
    "\n",
    "# This gives exact same thing as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef943b3a",
   "metadata": {},
   "source": [
    "### Above - there are 2 things to be learnt\n",
    "1. When direct indexing into matrix C\n",
    "    - It can be seen as the embedding matrix of characters\n",
    "    - Those embeddings of corresponding chars can be fed into the first layer of the network\n",
    "2. Instead, we can take **One-Hot** repr of characters:\n",
    "    - Matrix multiply them to C, which can be thought of as weights of first layer (also called embed layer)\n",
    "    - The result of this would be identical to the approach 1.\n",
    "\n",
    "### We're going to just use the 1st approach, index into the C to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ae7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not straight forward to index simultaneoudly for (32,3) matrix of integers from C\n",
    "# We have to use pytorch indexing \n",
    "\n",
    "# We can index using list\n",
    "\n",
    "C[[5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc88929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch indexing also works with tensor\n",
    "\n",
    "C[torch.tensor([5,6,7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also repeat a row multiple times\n",
    "\n",
    "C[torch.tensor([5,6,7,7,7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f86445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importantly, we can also index with multi-dimensional tensor\n",
    "\n",
    "# C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X].shape\n",
    "\n",
    "# This way we can embed all integers in X into 2 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61600f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438daad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X][13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above is same as\n",
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100)) #6 - 2-d embeddings for 3 blocks ; 100 - number of neurons taken in 1st hidden layer\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would ideally want to do \"Wx + b\"\n",
    "\n",
    "# emb @ W1 + b1\n",
    "\n",
    "# But this would not directly work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836b229",
   "metadata": {},
   "source": [
    "- We need to transform the tensor **emb** into a form\n",
    "    - such that **matrix multiplication** can work with **W1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways to do it are shown here:\n",
    "\n",
    "emb.view(32,6) == torch.cat(torch.unbind(emb, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.view(32,6).shape, torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5800d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence, we can do Wx + b as this\n",
    "\n",
    "# emb.view(32,6) @ W1 + b1\n",
    "#This has a problem that we are hard-coding first dimenssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ways to do the transformation are:\n",
    "\n",
    "print(emb.view(emb.shape[0], 6).shape)\n",
    "# or\n",
    "print(emb.view(-1, 6).shape) # This just calculates what must be the number of first dimension, given the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d728d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore: we now calculate hidden layer activations\n",
    "\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90898c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a77293",
   "metadata": {},
   "outputs": [],
   "source": [
    "(counts.sum(1, keepdim=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187de4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522ef7d",
   "metadata": {},
   "source": [
    "### Now we,\n",
    "    - index into rows of probs\n",
    "        - and pluck out probabilities assigned to the correct character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee834c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7621d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Doing all of above: TOGETHER --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a379c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b45080",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters in model\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef364ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    # ------------Below 3 lines can be very efficiently done with PyTorch in-built function----\n",
    "    # Also F.cross_entropy can be more efficient in forward, backward passes & numerically much well behaved\n",
    "    # counts = logits.exp()\n",
    "    # probs = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = probs[torch.arange(32), Y].log().mean()\n",
    "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4456d",
   "metadata": {},
   "source": [
    "### We are so easily able to overfit \n",
    "    - Coz we have so less data samples (32) to train with\n",
    "        - And we have a lot parameters (~4000) as per data count\n",
    "    - We are able to reduce to 0, but not there yet\n",
    "        - Coz, the starting data from every word --> '...' ----> e/o/... are not very deterministic to learn for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict teh next one\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    \n",
    "#     print(w)\n",
    "    context = [0] * block_size # padding with .'s of block_size count\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "#         print(''.join(itos[i] for i in context), '----->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop & append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e881b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters in model\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f45e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28846842",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X] # (228146, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c709a",
   "metadata": {},
   "source": [
    "1. Here we see, significant time taken for each epoch\n",
    "    - Which is due to us doing backprop once for entire data, which is big now.\n",
    "    - So we should think of trying **mini batch**\n",
    "        - We select some portion of dataset -> called **mini batch**\n",
    "            - Then do forward, backward and updates on that mini batch\n",
    "        - Repeat above for other portions (or mini batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10207e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torch.randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffd04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 10, size = (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, X.shape[0], size=(32,))\n",
    "# getting tensor of 32 (which is our batch_size) integers between (0, number of samples in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39438c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe78ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2) again it becomes a mini-batch of size 32\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "#     print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3bd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65794d9",
   "metadata": {},
   "source": [
    "1. But the training is much faster\n",
    "2. Now, due to minibatch, quality of our gradients is not as good\n",
    "    - coz, direction of gradient is not as good as when calculating over entire dataset\n",
    "    - **Meanwhile, in the start, the approximate direction with more steps (faster) is important than correct direction and lesser steps (slower)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab48e9",
   "metadata": {},
   "source": [
    "### Finding a decent Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000) # learning rate exponents - will give 1000 mumbers b/w (-3, 0)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8badb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9576f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd05f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2) again it becomes a batch of size 32\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "#     print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())\n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lre, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cd8a8",
   "metadata": {},
   "source": [
    "### These plots show --> Learning Rate around ~0.1 / LR exponent around -1 are getting best losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lri = []\n",
    "# lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2) again it becomes a batch of size 32\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "#     print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "#     lr = lrs[i]\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "#     # track stats\n",
    "#     lri.append(lr)\n",
    "#     lossi.append(loss.item())\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87797999",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00811b95",
   "metadata": {},
   "source": [
    "### We have surpassed bi-gram LM loss already, with around 40,000 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6772323",
   "metadata": {},
   "source": [
    "#### Once the loss reduction slows down, we try WEIGHT DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2) again it becomes a batch of size 32\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "#     print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7748df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # (32, 3, 2) again it becomes a batch of size 32\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c96c8",
   "metadata": {},
   "source": [
    "This is summary of how things go in training a model\n",
    " - We find a decent learning rate to start with (do a bit of training)\n",
    " - Do WEIGHT DECAY (say by a factor of 10) - and train again for a bit\n",
    " \n",
    "And we have a trained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49213d03",
   "metadata": {},
   "source": [
    "### Moving on to train / test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f2f8d",
   "metadata": {},
   "source": [
    "1. Above we had neural net with only ~4000 parameters, but as we go along and our parameters scale up\n",
    "    - The overfitting chances skyrocket (if data is not too big)\n",
    "        - Therefore, the very low **loss** on train data will not imply good performance\n",
    "            - Instead, it would mean the model started memorizing the data\n",
    "2. To remedy this, we has to have a held out set to evaluate performance of our models post training.\n",
    "    - training split, dev/validation split, test split\n",
    "        - 80%, 10%, 10%\n",
    "3. **Training split** - is to train the model parameters\n",
    "4. **Dev/validation split** - to train model hyper-parameters\n",
    "5. **Test split** - to test model performance\n",
    "    - But we must **NOT** evaluate loss of the model on **test split** frequently.\n",
    "        - Coz everytime you evaluate on test split and learn something from it\n",
    "            - then the model starts learning from this data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bed6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "\n",
    "    block_size = 3 # context length: how many characters do we take to predict teh next one\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size # padding with .'s of block_size count\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "    #         print(''.join(itos[i] for i in context), '----->', itos[ix])\n",
    "            context = context[1:] + [ix] # crop & append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38700be",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,300), generator = g)\n",
    "b1 = torch.randn(300, generator = g)\n",
    "W2 = torch.randn((300,27), generator = g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e46319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total parameters in model\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafae578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(30000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2) again it becomes a batch of size 32\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    #print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "        \n",
    "# print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cce418",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[Xtr] # (32, 3, 2) again it becomes a batch of size 32\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[Xdev] # (32, 3, 2) again it becomes a batch of size 32\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ed6c6",
   "metadata": {},
   "source": [
    "### Before moving on to address the bottleneck of low dimensions of embeddings\n",
    "    - Let's visualize the embeddings in 2-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6248a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha='center', va='center', color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e664d",
   "metadata": {},
   "source": [
    "### Now let's try with more number of embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 20), generator=g)\n",
    "W1 = torch.randn((60, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200000):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 10)\n",
    "    h = torch.tanh(emb.view(-1, 60) @ W1 +b1) # (32, 200)\n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "#     lr = 0.01\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "    # we are using log10 here to squash the hockey stick appearance of plot; prints log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss over training set\n",
    "\n",
    "emb = C[Xtr] # (32, 3, 2) again it becomes a batch of size 32\n",
    "h = torch.tanh(emb.view(-1, 60) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0263e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss over dev set\n",
    "\n",
    "emb = C[Xdev] # (32, 3, 2) again it becomes a batch of size 32\n",
    "h = torch.tanh(emb.view(-1, 60) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "\n",
    "loss # Looking at the loss on entire training data after some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc91c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
