{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43089d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrigank/miniconda3/envs/fast/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a11570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e59b0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6380da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocab of characters & integers\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7730ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6cee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 20\n",
    "n_hidden = 200\n",
    "\n",
    "batch_size = 32\n",
    "max_steps = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2333fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18167"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509f5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031ae989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3032\n",
      "  10000/ 200000: 2.0484\n",
      "  20000/ 200000: 2.8247\n",
      "  30000/ 200000: 2.2985\n",
      "  40000/ 200000: 2.1025\n",
      "  50000/ 200000: 2.4764\n",
      "  60000/ 200000: 2.6015\n",
      "  70000/ 200000: 2.3140\n",
      "  80000/ 200000: 1.9098\n",
      "  90000/ 200000: 2.0857\n",
      " 100000/ 200000: 1.7765\n",
      " 110000/ 200000: 1.9925\n",
      " 120000/ 200000: 1.9428\n",
      " 130000/ 200000: 2.2242\n",
      " 140000/ 200000: 2.1578\n",
      " 150000/ 200000: 2.0473\n",
      " 160000/ 200000: 2.4547\n",
      " 170000/ 200000: 2.1142\n",
      " 180000/ 200000: 1.6728\n",
      " 190000/ 200000: 1.8863\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fea08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0070888996124268\n",
      "val 2.0923452377319336\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e982073",
   "metadata": {},
   "source": [
    "## Issue with scaling manually all the W and b parameters - to ensure unit Gaussian nature throughout the network\n",
    "\n",
    "- It's hard to decide scaling when it comes to networks with large number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeca66f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a962b1",
   "metadata": {},
   "source": [
    "# Precise setting of such scalings, is not that important: Due to some modern innovations.\n",
    "\n",
    "1. Batch Norm (https://arxiv.org/pdf/1502.03167.pdf)\n",
    "    - Idea was, if we wish to keep the activations throughout the network in **unit Gaussian** range\n",
    "        - Why don't just **normalize** then with mean & std, which would maintain the desired distribution\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87cdda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of the activations over a batch: so a neuron will have a mean & std calculated across batch examples\n",
    "hpreact.mean(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0cab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.std(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cdccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# Scaling params for batch-norm\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "# parameters = [C, W1, b1, W2, b2]\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fbbc2f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2924\n",
      "  10000/ 200000: 2.2866\n",
      "  20000/ 200000: 2.2139\n",
      "  30000/ 200000: 2.1443\n",
      "  40000/ 200000: 2.3116\n",
      "  50000/ 200000: 1.9430\n",
      "  60000/ 200000: 2.4954\n",
      "  70000/ 200000: 2.3500\n",
      "  80000/ 200000: 2.4079\n",
      "  90000/ 200000: 2.3009\n",
      " 100000/ 200000: 2.0993\n",
      " 110000/ 200000: 1.9806\n",
      " 120000/ 200000: 1.6657\n",
      " 130000/ 200000: 1.9281\n",
      " 140000/ 200000: 2.3242\n",
      " 150000/ 200000: 2.4478\n",
      " 160000/ 200000: 2.0123\n",
      " 170000/ 200000: 2.4992\n",
      " 180000/ 200000: 2.2298\n",
      " 190000/ 200000: 2.1871\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    \n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    ## Normalizing the hpreact, with it's mean & std\n",
    "    ## This would not give good performance yet\n",
    "    ## Coz, we want the activation firings to be roughly \"unit Gaussian\" at initialization\n",
    "    ## But, DON'T want then to be forced Gaussian always\n",
    "    \n",
    "#     hpreact = embcat @ W1 + b1\n",
    "#     hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True)\n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    ## To handle not standardising (normalizing) always:\n",
    "    ## Paper introduces an additional component - \"Scaling & shifting\"\n",
    "        ## Take normalized inputs:\n",
    "            ## normalize then with some GAIN\n",
    "            ## offset with some BIAS\n",
    "                ## Also, due to this, the b1 bias added below, would not serve any purpose\n",
    "    \n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    hpreact = (bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)) + bnbias\n",
    "    ## This above treatment to hpreact will make\n",
    "    ## all neurons firing at unit Gaussian - at INITIALIZATION\n",
    "        ## As bngain is 1s, bnbias is 0s\n",
    "    ## And as optimization starts - they will start to adjust\n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3732954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f46e0121940>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e105061e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.044732093811035\n",
      "val 2.0971972942352295\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # h = torch.tanh(embcat @ W1 + b1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    # hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a116766",
   "metadata": {},
   "source": [
    "## We will not see any improvement in performance\n",
    "- Coz, we are dealing with very small NN right now\n",
    "- Also, we already had scaled W's decently, so **Batch Norm** does not have much to do here\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ac3c6",
   "metadata": {},
   "source": [
    "## In future scenarios, where we'll have much larger networks - with Residual connections, CNNs etc, setting scales for all W matrices will become intractable\n",
    "- And then we can see **Batch Norm** giving gains\n",
    "- Then careful scaling of **W's** initialization would not be required\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eeec7",
   "metadata": {},
   "source": [
    "## Batch Norm has a COST attached to it\n",
    "- Till now, an example was only depending on model parameters to produce its logits.\n",
    "    - But with introduction of **Batch Norm** through a batch\n",
    "        - A mathematical coupling of examples happens in forward & backward pass\n",
    "- So, now the logits produced by an example is not only a function of its inputs & model params\n",
    "    - But also, of other examples in the **batch**\n",
    "- Suppose an example's **h** is going to jitter according to the samples that come into the batch with it\n",
    "\n",
    "### It might seem like a BUG / UNDESIRABLE\n",
    "### But it will turn out (as we'll see) as some kind of REGULARIZATION effect\n",
    "- Also some kind of **data augmentation**\n",
    "\n",
    "### This property of mathematical coupling b/w examples in a batch is still UNDESIRABLE\n",
    "- So there exist other form of normalization methods, where normalizing across batch is not done\n",
    "    - Layer Norm\n",
    "    - Instance Norm\n",
    "    - Group Norm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c082",
   "metadata": {},
   "source": [
    "# ??? Problem: \n",
    "- How to we feed in single example during TEST to generate logits\n",
    "- Coz, the network is now trained to calculate (mean, std) over a batch for **Batch Norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e01b1",
   "metadata": {},
   "source": [
    "## Solution (proposed in paper):\n",
    "- Fix and clamp a **bnmean** and **bnstd**\n",
    "    Now we can pass a single test example to test the performance\n",
    "- 2 ways to do it:\n",
    "    1. After **training** completes, find a fix value for both and clamp them for testing purpose\n",
    "    2. Instead calculate both in a **running** manner while training\n",
    "        - On the side of training, **we calculate running mean & std of batch norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b35f032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18367"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# Scaling params for batch-norm\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # Coz, initially hpreact will come out to be unit Gaussian, by the way of W init\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "# parameters = [C, W1, b1, W2, b2]\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1522d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2924\n",
      "  10000/ 200000: 2.2866\n",
      "  20000/ 200000: 2.2139\n",
      "  30000/ 200000: 2.1443\n",
      "  40000/ 200000: 2.3116\n",
      "  50000/ 200000: 1.9430\n",
      "  60000/ 200000: 2.4954\n",
      "  70000/ 200000: 2.3500\n",
      "  80000/ 200000: 2.4079\n",
      "  90000/ 200000: 2.3009\n",
      " 100000/ 200000: 2.0993\n",
      " 110000/ 200000: 1.9806\n",
      " 120000/ 200000: 1.6657\n",
      " 130000/ 200000: 1.9281\n",
      " 140000/ 200000: 2.3242\n",
      " 150000/ 200000: 2.4478\n",
      " 160000/ 200000: 2.0123\n",
      " 170000/ 200000: 2.4992\n",
      " 180000/ 200000: 2.2298\n",
      " 190000/ 200000: 2.1871\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    \n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # mostly what it was earlier, just a small update in the direction of mean of batch\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        \n",
    "        # mostly what it was earlier, just a small update in the direction of std of batch\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.0001 * bnstdi\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70babdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.9740841388702393\n",
      "val 3.0694797039031982\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # h = torch.tanh(embcat @ W1 + b1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "#     hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bbbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
