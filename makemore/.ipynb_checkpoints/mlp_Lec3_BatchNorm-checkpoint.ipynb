{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43089d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrigank/miniconda3/envs/fast/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a11570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e59b0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6380da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocab of characters & integers\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7730ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6cee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 20\n",
    "n_hidden = 200\n",
    "\n",
    "batch_size = 32\n",
    "max_steps = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2333fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509f5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031ae989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3032\n",
      "  10000/ 200000: 2.0484\n",
      "  20000/ 200000: 2.8247\n",
      "  30000/ 200000: 2.2985\n",
      "  40000/ 200000: 2.1025\n",
      "  50000/ 200000: 2.4764\n",
      "  60000/ 200000: 2.6015\n",
      "  70000/ 200000: 2.3140\n",
      "  80000/ 200000: 1.9098\n",
      "  90000/ 200000: 2.0857\n",
      " 100000/ 200000: 1.7765\n",
      " 110000/ 200000: 1.9925\n",
      " 120000/ 200000: 1.9428\n",
      " 130000/ 200000: 2.2242\n",
      " 140000/ 200000: 2.1578\n",
      " 150000/ 200000: 2.0473\n",
      " 160000/ 200000: 2.4547\n",
      " 170000/ 200000: 2.1142\n",
      " 180000/ 200000: 1.6728\n",
      " 190000/ 200000: 1.8863\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fea08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0070888996124268\n",
      "val 2.0923452377319336\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(embcat @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e982073",
   "metadata": {},
   "source": [
    "## Issue with scaling manually all the W and b parameters - to ensure unit Gaussian nature throughout the network\n",
    "\n",
    "- It's hard to decide scaling when it comes to networks with large number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeca66f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a962b1",
   "metadata": {},
   "source": [
    "# Precise setting of such scalings, is not that important: Due to some modern innovations.\n",
    "\n",
    "1. Batch Norm (https://arxiv.org/pdf/1502.03167.pdf)\n",
    "    - Idea was, if we wish to keep the activations throughout the network in **unit Gaussian** range\n",
    "        - Why don't just **normalize** then with mean & std, which would maintain the desired distribution\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f87cdda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of the activations over a batch: so a neuron will have a mean & std calculated across batch examples\n",
    "hpreact.mean(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d0cab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.std(dim=0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cdccac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# Scaling params for batch-norm\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "# parameters = [C, W1, b1, W2, b2]\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fbbc2f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2924\n",
      "  10000/ 200000: 2.2866\n",
      "  20000/ 200000: 2.2139\n",
      "  30000/ 200000: 2.1443\n",
      "  40000/ 200000: 2.3116\n",
      "  50000/ 200000: 1.9430\n",
      "  60000/ 200000: 2.4954\n",
      "  70000/ 200000: 2.3500\n",
      "  80000/ 200000: 2.4079\n",
      "  90000/ 200000: 2.3009\n",
      " 100000/ 200000: 2.0993\n",
      " 110000/ 200000: 1.9806\n",
      " 120000/ 200000: 1.6657\n",
      " 130000/ 200000: 1.9281\n",
      " 140000/ 200000: 2.3242\n",
      " 150000/ 200000: 2.4478\n",
      " 160000/ 200000: 2.0123\n",
      " 170000/ 200000: 2.4992\n",
      " 180000/ 200000: 2.2298\n",
      " 190000/ 200000: 2.1871\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    \n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    ## Normalizing the hpreact, with it's mean & std\n",
    "    ## This would not give good performance yet\n",
    "    ## Coz, we want the activation firings to be roughly \"unit Gaussian\" at initialization\n",
    "    ## But, DON'T want then to be forced Gaussian always\n",
    "    \n",
    "#     hpreact = embcat @ W1 + b1\n",
    "#     hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True)\n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    ## To handle not standardising (normalizing) always:\n",
    "    ## Paper introduces an additional component - \"Scaling & shifting\"\n",
    "        ## Take normalized inputs:\n",
    "            ## normalize then with some GAIN\n",
    "            ## offset with some BIAS\n",
    "                ## Also, due to this, the b1 bias added below, would not serve any purpose\n",
    "    \n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    hpreact = (bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)) + bnbias\n",
    "    ## This above treatment to hpreact will make\n",
    "    ## all neurons firing at unit Gaussian - at INITIALIZATION\n",
    "        ## As bngain is 1s, bnbias is 0s\n",
    "    ## And as optimization starts - they will start to adjust\n",
    "    ## -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3732954b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb232f8e6d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e105061e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.044732093811035\n",
      "val 2.0971972942352295\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # h = torch.tanh(embcat @ W1 + b1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    # hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a116766",
   "metadata": {},
   "source": [
    "## We will not see any improvement in performance\n",
    "- Coz, we are dealing with very small NN right now\n",
    "- Also, we already had scaled W's decently, so **Batch Norm** does not have much to do here\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ac3c6",
   "metadata": {},
   "source": [
    "## In future scenarios, where we'll have much larger networks - with Residual connections, CNNs etc, setting scales for all W matrices will become intractable\n",
    "- And then we can see **Batch Norm** giving gains\n",
    "- Then careful scaling of **W's** initialization would not be required\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eeec7",
   "metadata": {},
   "source": [
    "## Batch Norm has a COST attached to it\n",
    "- Till now, an example was only depending on model parameters to produce its logits.\n",
    "    - But with introduction of **Batch Norm** through a batch\n",
    "        - A mathematical coupling of examples happens in forward & backward pass\n",
    "- So, now the logits produced by an example is not only a function of its inputs & model params\n",
    "    - But also, of other examples in the **batch**\n",
    "- Suppose an example's **h** is going to jitter according to the samples that come into the batch with it\n",
    "\n",
    "### It might seem like a BUG / UNDESIRABLE\n",
    "### But it will turn out (as we'll see) as some kind of REGULARIZATION effect\n",
    "- Also some kind of **data augmentation**\n",
    "\n",
    "### This property of mathematical coupling b/w examples in a batch is still UNDESIRABLE\n",
    "- So there exist other form of normalization methods, where normalizing across batch is not done\n",
    "    - Layer Norm\n",
    "    - Instance Norm\n",
    "    - Group Norm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c082",
   "metadata": {},
   "source": [
    "# ??? Problem: \n",
    "- How to we feed in single example during TEST to generate logits\n",
    "- Coz, the network is now trained to calculate (mean, std) over a batch for **Batch Norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e01b1",
   "metadata": {},
   "source": [
    "## Solution (proposed in paper):\n",
    "- Fix and clamp a **bnmean** and **bnstd**\n",
    "    Now we can pass a single test example to test the performance\n",
    "- 2 ways to do it:\n",
    "    1. After **training** completes, find a fix value for both and clamp them for testing purpose\n",
    "    2. Instead calculate both in a **running** manner while training\n",
    "        - On the side of training, **we calculate running mean & std of batch norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b35f032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18367"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/ 30**0.5 #0.15\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# Scaling params for batch-norm\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden)) # Coz, initially hpreact will come out to be unit Gaussian, by the way of W init\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "# parameters = [C, W1, b1, W2, b2]\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1522d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2924\n",
      "  10000/ 200000: 2.2866\n",
      "  20000/ 200000: 2.2139\n",
      "  30000/ 200000: 2.1443\n",
      "  40000/ 200000: 2.3116\n",
      "  50000/ 200000: 1.9430\n",
      "  60000/ 200000: 2.4954\n",
      "  70000/ 200000: 2.3500\n",
      "  80000/ 200000: 2.4079\n",
      "  90000/ 200000: 2.3009\n",
      " 100000/ 200000: 2.0993\n",
      " 110000/ 200000: 1.9806\n",
      " 120000/ 200000: 1.6657\n",
      " 130000/ 200000: 1.9281\n",
      " 140000/ 200000: 2.3242\n",
      " 150000/ 200000: 2.4478\n",
      " 160000/ 200000: 2.0123\n",
      " 170000/ 200000: 2.4992\n",
      " 180000/ 200000: 2.2298\n",
      " 190000/ 200000: 2.1871\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "    \n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # mostly what it was earlier, just a small update in the direction of mean of batch\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        \n",
    "        # mostly what it was earlier, just a small update in the direction of std of batch\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.0001 * bnstdi\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70babdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.9740841388702393\n",
      "val 3.0694797039031982\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # h = torch.tanh(embcat @ W1 + b1)\n",
    "    hpreact = embcat @ W1 # + b1\n",
    "#     hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bbbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
